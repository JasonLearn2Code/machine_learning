{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.2_Linear Regression from scratch_(Lab).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yShVC06ke7__"},"source":["# Lab - Linear Regression"]},{"cell_type":"code","metadata":{"id":"hT08pq4YfEPG"},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_style(\"whitegrid\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Myi94F9Nf8Lb"},"source":["from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1aXBTr6h5Xe"},"source":["## Linear Regression from scratch"]},{"cell_type":"markdown","metadata":{"id":"sP00MP9Ki4qf"},"source":["### Import data"]},{"cell_type":"code","metadata":{"id":"YL5OW2q0gqjp"},"source":["\n","df = pd.read_csv('https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/Advertising.csv',\n","                 index_col=0)\n","X = df[['TV']].values\n","y = df[['Sales']].values\n","\n","plt.scatter(X, y, alpha=0.6)\n","plt.xlabel('TV ads')\n","plt.ylabel('Sales')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D0WDzy_1bRYH"},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5KgfGC7ejvO6"},"source":["### Implement Linear Regression with sklearn"]},{"cell_type":"markdown","metadata":{"id":"eUd2VHeiNDGi"},"source":["We will build a Linear Regression model to predict `Sales` from the other features. Let's start with a Simple Linear Regression first: Use `TV` to predict `Sales`. We build the model using `sklearn.linear_model.LinearRegression` first, so that we can compare our result later on:"]},{"cell_type":"code","metadata":{"id":"4AQzCRFrjfXy"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","lg = LinearRegression()\n","lg.fit(X, y)\n","\n","print(f'Coef: {lg.coef_}')\n","print(f'Intercept: {lg.intercept_}')\n","print(f'MSE: {mean_squared_error(y, lg.predict(X))}')\n","\n","plt.scatter(X, y, alpha=0.6)\n","plt.plot(X, lg.predict(X), c='r')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0y5CfFQNlOZU"},"source":["### Implement from scratch\n","\n","Here are the main 5 steps of Linear Regression using Gradient Descent\n","\n","1. Initialize a value for $w$ and $b$. We will initialize them with zeros\n","2. Calculate predictions $\\hat{y}$ on all training observations (forward pass)\n","3. Calculate the loss value L($w$,$b$)\n","4. Find $\\frac{\\partial L}{\\partial w_j}$ and $\\frac{\\partial L}{\\partial b}$ (backward pass)\n","5. Update our parameters: $\\begin{cases} \n","w = w - \\alpha\\frac{\\partial L}{\\partial w} \\\\\n","b = b - \\alpha\\frac{\\partial L}{\\partial b}\n","\\end{cases}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1jNgXhtJSRCx"},"source":["Alright, let's do it!"]},{"cell_type":"markdown","metadata":{"id":"o2IsiLV-SKnJ"},"source":["**Step 1: Initialization**"]},{"cell_type":"code","metadata":{"id":"20l0SmhKj4kV"},"source":["# Initialization\n","def initialize_params(X):\n","    '''Initialize w and b with zeros and return them. Make sure to get the right shape for w and b'''\n","    # Your code here\n","    w = np.zeros((X.shape[1], 1))\n","    b = np.zeros((1, ))\n","    return w, b\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yt3EnQIXmo_m"},"source":["**Step 2: Making prediction (Forward Propagation)**\n","\n","\n","$$\n","\\hat{y} = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b = b + \\sum^n{w_ix_i} = w^Tx + b\n","$$\n","\n","but we will use vectorization form (matrix multiplication)\n","$$\n","\\hat{y} = Xw + b\n","$$"]},{"cell_type":"code","metadata":{"id":"rH3o1VBOlhfi"},"source":["# Function for forward propagation\n","def forward(X, w, b):\n","    '''Calculate and return y_hat'''\n","    # Your code here\n","    \n","    return X @ w + b \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UbnPiQWsniEf"},"source":["**Step 3: Loss function:**\n","\n","$$\n","L(w, b) = \\frac{1}{m} \\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})^2}\n","$$"]},{"cell_type":"code","metadata":{"id":"eYQ9H6QKmBNu"},"source":["# Funnction to calculate mean squared error\n","def mse(y_hat, y):\n","    # Your code here\n","    \n","    return ((y_hat - y)**2).mean() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5g7vrDLm5A8"},"source":["**Step 4: Calculating gradients (Backward Propagation)**\n","\n","Vectorization form\n","\n","$$\n","\\frac{\\partial L}{\\partial w} = \\frac{2}{m} X^T . (\\hat{y} - y)\n","$$\n","<br/>\n","$$\n","\\frac{\\partial L}{\\partial b} = \\frac{2}{m} \\sum_{i=1}^{m}{(\\hat{y}^{(i)} - y^{(i)})}\n","$$"]},{"cell_type":"code","metadata":{"id":"7Bw2pLs9lmRf"},"source":["# Function for backward propagation\n","def backward(X, y, y_hat, w, b):\n","    '''Calculate dw, db and return them'''\n","    m = X.shape[0]\n","    dw = 2/m * X.T @ (y_hat - y)\n","    db = 2/m * np.sum(y_hat - y)\n","    \n","    return dw, db\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8tU7OOraTPPY"},"source":["**Step 5: Update parameters**\n"]},{"cell_type":"markdown","metadata":{"id":"cLKQKKJyUcuS"},"source":["Update our parameters: $\\begin{cases} \n","w = w - \\alpha\\frac{\\partial L}{\\partial w} \\\\\n","b = b - \\alpha\\frac{\\partial L}{\\partial b}\n","\\end{cases}$"]},{"cell_type":"code","metadata":{"id":"x4WslH_rTToT"},"source":["def update_params(w, b, dw, db, learning_rate):\n","    '''Update w, b and return them'''\n","    # Your code here\n","    w = w - learning_rate * dw\n","    b = b - learning_rate * db\n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NuuL-l_4oEoW"},"source":["**Train model**"]},{"cell_type":"code","metadata":{"id":"CmEtiPMLoSS-"},"source":["# Function for training model\n","def train(X, y, iterations, learning_rate):\n","    '''Train w, b and return'''\n","    # Your code here\n","\n","    # Step 1: initialize the parameters\n","    w,b = initialize_params(X)\n","\n","    for i in range(iterations):\n","        # Step 2: forward pass\n","        y_hat = forward(X, w, b)\n","        # Step 3: calculate loss\n","        L = mse(y_hat, y)\n","        # Step 4: backward pass\n","        dw, db = backward(X, y, y_hat, w, b)\n","        # Step 5: update params\n","        w, b = update_params(w, b, dw, db, learning_rate)\n","\n","        if i % 100 == 0:\n","            print(f'Step {i}, MSE = {L}')\n","\n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfqTVaF4NDGw"},"source":["# Setup learning rate & number of iterations\n","# Your code here\n","learning_rate = 0.00001\n","iterations = 100000\n","# train the model. Use the train function you created above\n","# Your code here\n","w, b = train(X, y, iterations=iterations, learning_rate=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkiOyrkkozPb"},"source":["**Evaluation**"]},{"cell_type":"code","metadata":{"id":"fNpj0z_TNDGz"},"source":["# Prediction\n","def predict(X, w, b):\n","    '''Return predicted y with the input X'''\n","    return forward(X,w,b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BsQCzQTo4dy"},"source":["# Calculate y_hat from X with the parameters w, b that you have trained\n","# Print out the MSE between y_hat and y\n","y_hat = predict(X, w, b)\n","print(mse(y_hat, y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNwEcBgqg-kQ"},"source":["# Plot the data with your model\n","plt.scatter(X, y, alpha=0.5) # the data\n","plt.plot(X, y_hat, c='r') # the prediction\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqY8A8uQhQa6"},"source":["# Print out the value of w and b\n","print('Coef:', w)\n","print('Intercept:', b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtrG7kdejBQ3"},"source":["# Output from sklearn model. Try to match your MSE with sklearn's\n","# Coef: [[0.04753664]]\n","# Intercept: [7.03259355]\n","# MSE: 10.512652915656757"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pllm5sizmoMP"},"source":["## Multiple Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"dfQcpQiUNDG_"},"source":["Now let's move on to use all of the features to predict `Sales`"]},{"cell_type":"code","metadata":{"id":"i9O8EIwGmqxi"},"source":["X = df[['TV', 'Radio', 'Newspaper']].values\n","y = df[['Sales']].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KBQER1riBALv"},"source":["# Standardization\n","x_mean = np.mean(X, axis = 0)\n","x_std = np.std(X, axis = 0)\n","X_scaled = (X - x_mean)/x_std"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgnc0I5ENDHD"},"source":["# Train the model\n","learning_rate = 1e-2\n","iterations = 1000\n","\n","w, b = train(X_scaled, y, iterations=iterations, learning_rate=learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jR8rTWJPnHYJ"},"source":["# Print out w and b\n","print('Coef:', w) # only for X_scaled, not X\n","print('Intercept:', b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2Th79CnnJ19"},"source":["# Print out the mse of the model after training\n","mse(predict(X_scaled, w, b), y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQw8qSVHnMHI"},"source":["# Compare with the model from sklearn\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","print(f'MSE: {mean_squared_error(y, model.predict(X))}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0yWYGgZXYz_"},"source":["# Standardization"]},{"cell_type":"code","metadata":{"id":"mpOUmCWtDunD"},"source":["df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4iWdNcSxXZ99"},"source":["Visualization of loss landscape: standardized vs unstandardized\n","\n","\n","![](https://i.imgur.com/4Vdlq5l.png)"]}]}