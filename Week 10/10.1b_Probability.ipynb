{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10.1a_Probability.ipynb","provenance":[{"file_id":"11pwWQwYb2gR9JObY_JHNv29YHZIpIy6W","timestamp":1621244656224}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5zGKfUJmqaOc"},"source":["# Basic Probability"]},{"cell_type":"markdown","metadata":{"id":"m2eOaIbNsjT3"},"source":["We often quantify uncertainty in the data, uncertainty in the ML model, and uncertainty in the predictions produced by the model. Probability is the way of **quantifying the uncertainty**. The theory of probability aims at defining a **mathematical structure** to **describe random outcomes of experiments**. That requires the idea of **random variables**: a **function that maps outcomes of random experiments to a set of properties (numbers)** that we are interested in. \n","\n","A random process (flipping a coin, asking a girl out): outcome => numbers\n","\n","Process: ask a girl out => 1 if she says yes, 0 if she says no\n","\n","Associated with the random variable is a **function that measures the probability that a particular outcome or set of outcomes will occur** called the **probability distribution function**."]},{"cell_type":"markdown","metadata":{"id":"J-otgKzNw_vR"},"source":["![](https://img-9gag-fun.9cache.com/photo/aAD4p79_700bwp.webp)\n","\n","Using probability, we can consider a model of some process, where the underlying uncertainty is captured by random variables, and we use the rules of probability to derive what happens. In statistics, we observe that **something has happened** and try to figure out the **underlying process that explains the observations**."]},{"cell_type":"code","metadata":{"id":"83FgkwhhqHnZ","executionInfo":{"status":"ok","timestamp":1621245520452,"user_tz":-420,"elapsed":3258,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"}}},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from functools import partial\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_style(\"whitegrid\")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RVmi7hW1Spl"},"source":["In general, to calculate the probability of an event happening:\n","$$\n","\\frac{\\text{number of ways it can happen}}{\\text{total number of outcomes}}\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"m-Do4t5B1rpn"},"source":["Example:\n","- Throwing dices: when a single dice is thrown, there are 6 possible outcomes (1,2,3,4,5,6)\n","- The probability to roll any value in those outcomes is 1/6"]},{"cell_type":"markdown","metadata":{"id":"zA9y4v2h4XHL"},"source":["![](https://i.imgur.com/7c1Zllz.png)"]},{"cell_type":"markdown","metadata":{"id":"CpFFtBB42idX"},"source":["Question:\n","\n","A dice is thrown once. What is the probability that the score is a factor of 6\n","- A. 1/6\n","- B. 1/2\n","- C. 2/3\n","- D. 1"]},{"cell_type":"markdown","metadata":{"id":"dhZoaHKLwPbI"},"source":["## Basic"]},{"cell_type":"markdown","metadata":{"id":"lNsUnVJUwU-2"},"source":["**The sample space** $\\Omega$ is a fixed set of all possible outcomes. **The probability** $P(A)$ measures the probability that the event $A$ will occur.\n","\n","* $P(\\Omega) = 1$\n","* $0 \\leq P(A) \\leq 1$\n","* The **compliment** of $A$ is $A^C$, and $P(A^C) = 1 - P(A)$\n","* If $A$ and $B$ are events, the $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$\n","![](https://studywell.com/wp-content/uploads/2019/10/Venn-420x300.png)\n","* Two events $A$ and $B$ are **dependent** if knowing something about whether $A$ happens gives us information about whether $B$ happens(and vice versa). Otherwise, they are **independent**."]},{"cell_type":"markdown","metadata":{"id":"higqaPhvmqOn"},"source":["## Expected Value"]},{"cell_type":"markdown","metadata":{"id":"86zmwRQS7gGP"},"source":["\n","\n","The **expected value** of a random variable is **the sum (or intergrating) of the possible outcomes weighted by their probability**. It can be interpreted as the **long-run average of many independent samples from the given distribution.**\n","\n","Expected value is defined as\n","\n","$$\n","E[X] = \\sum_{x \\in X(\\Omega)}{x}{p(x)}\n","$$\n","\n","for discrete $X$ and as\n","\n","$$\n","E[X] = \\int_{-\\infty}^{\\infty}{x}{p(x)dx}\n","$$\n","\n","for continuous X.\n","\n","The expected value has a physical interpretation as the \"center of mass\" of the distribution. "]},{"cell_type":"markdown","metadata":{"id":"B31pWbZWw_v-"},"source":["<img src=\"https://www.mathwords.com/e/e_assets/e41.gif\" width=\"800px\"/>"]},{"cell_type":"markdown","metadata":{"id":"KWlRzoq9YBCS"},"source":["[Visualization](https://seeing-theory.brown.edu/basic-probability/index.html)"]},{"cell_type":"code","metadata":{"id":"zWkOV_fCw_wA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607940183835,"user_tz":-420,"elapsed":929,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"}},"outputId":"4f00db2f-71c8-41ee-e230-f9b50ad3c7e6"},"source":["# X: random variable of a dice draw: ranging from 1 to 6\n","p =1/6\n","X = np.arange(1,7)\n","print(X)\n","E_X = np.sum([p*x for x in X])\n","print(E_X)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1 2 3 4 5 6]\n","3.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TAqaZe679-tA"},"source":["The **variance** measures the dispersion\n","\n","$$\n","Var(X) = E[(X - E[X])^2]\n","$$\n","\n","![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4ad35c4161b9cf52868e879d457d8d796094ff02)"]},{"cell_type":"code","metadata":{"id":"3wJnFWVGw_wN"},"source":["X1 = np.array([33,34,35,37,39]) # 35 (+-)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbBKkc-jw_wS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607940371839,"user_tz":-420,"elapsed":1369,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"}},"outputId":"6ae4a5a8-66a4-41e2-bb3d-15d5a00665ce"},"source":["np.mean(X1**2) - np.mean(X1)**2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.639999999999873"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"FYXxGSg-w_wX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619514240909,"user_tz":-420,"elapsed":586,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"}},"outputId":"1e0b9c07-c085-414b-e725-16484b0ec0a1"},"source":["np.var(X1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.64"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"ZIaMtko6w_wb"},"source":["Variance is a useful notion, but **it suffers from that fact the units of variance are not the same as the units of the random variable** ( because of the squaring). To overcome this problem we can use **standard deviation**, which is defined as $\\sqrt{Var(X)}$. The standard deviation of X has the same units as X.\n"]},{"cell_type":"markdown","metadata":{"id":"d4-Ce6M7HSix"},"source":["## Distributions\n","\n","There are two major classes of probability distributions: **discrete**, or **continuous**. \n","\n","A **random variable** $X$ is a function that assigns a real number to each outcome in the probability space. \n","- When $X$ can take only a finite number of values, so it is known as a **discrete random variable**. \n","- When $X$ takes on a infinite number of possible values, so it is called a **continuous random variable**. \n","\n","[Visualization](https://seeing-theory.brown.edu/probability-distributions/index.html#section1)\n","\n","If X is a continuous random variable, then there exists unique nonnegative functions, $f(x)$ (**probability density function PDF**) and $F(x)$ (**cumulative distribution function CDF**), such that the following are true:\n","\n","$$\n","P(a \\leq X \\leq b) = \\int_{a}^{b}{f(x)dx} \\\\\n","P(X < x) = F(x)\n","$$\n","\n","If $X$ is discrete, $f(x)$ is called **probability mass function PMF**:\n","\n","$$\n","P(X=x) = f(x) \\\\\n","P(X<x) = F(x)\n","$$\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fwjNwdHELwON"},"source":["Some common discrete distributions:\n","\n","* **Bernoulli(p)** (where $0 \\leq p \\leq 1$): one if a coin with heads probability p comes up heads, zero otherwise.\n","$$\n","f(x) = \\begin{cases}\n","    p & if\\ x = 1 \\\\\n","    1âˆ’p & if\\ x = 0\n","\\end{cases}\n","$$\n","\n","* **Binomial(n, p)** (where $0 \\leq p \\leq 1$): the number of heads in n independent flips of a coin with heads probability p."]},{"cell_type":"markdown","metadata":{"id":"Pw7b8Yggw_wh"},"source":["$$\n","f(x) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} p^k(1-p)^{n-k}\n","$$\n","\n","with\n","\n","$$\n","\\begin{pmatrix} n \\\\ k \\end{pmatrix} = \\frac{n!}{k!(n-k)!}\n","$$"]},{"cell_type":"code","metadata":{"id":"4vgXwlAHw_wi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619516486842,"user_tz":-420,"elapsed":604,"user":{"displayName":"Quan Tran","photoUrl":"","userId":"11078860236930939203"}},"outputId":"0d76bf7d-b458-474c-c3c1-098bf402827c"},"source":["# Example: A car company with defective rate of 0.3 (30%). \n","\n","\n","p=0.3\n","# What is the probability of getting 1 defective car out of 3 cars\n","\n","n=3 # number of cars in total\n","k=1\n","\n","# number of ways to have 1 defective car out of 3 cars (combination)\n","# bad good good, good bad good, good good bad\n","comb = 3\n","\n","\n","\n","comb * (p**k) * (1-p)**(n-k)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4409999999999999"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"HZeUWDeVw_wm"},"source":["Some common continuous distributions:\n","\n","* **Uniform(a,b)** (where a < b): equal probability density to every value between a and b on the real line.\n","\n","$$\n","f(x) = \\begin{cases}\n","\\frac{1}{b-a} & if\\ a \\leq x \\leq b \\\\\n","0 & otherwise\n","\\end{cases}\n","$$\n","\n","* **Normal($\\mu$, $\\sigma^2$)**, also known as the Gaussian distribution\n","\n","$$\n","f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma}}e^{-\\frac{1}{2\\sigma^2}{(x-\\mu)^2}}\n","$$\n","\n","[Visualization](https://seeing-theory.brown.edu/probability-distributions/index.html)"]},{"cell_type":"markdown","metadata":{"id":"iE3GwdZMPfK5"},"source":["## Central limit theorem\n","\n","The Central Limit Theorem (CLT) states that the sample mean of a sufficiently large number of i.i.d. random variables is approximately normally distributed. The larger the sample, the better the approximation.\n","\n","\n","When you take a sample with large enough observations (30 is sufficient) from a population and calculate sample mean, and repeat this procedure several times (>100), those means will form a normal distribution even though the population are not normally distributed.\n","\n","[Visualization](https://seeing-theory.brown.edu/probability-distributions/index.html)"]},{"cell_type":"markdown","metadata":{"id":"ZtOpZdxNw_wo"},"source":["## Conditional probability (Bayes Theorem)\n","\n","The conditional probability of event $A$ given that event $B$ has occurred is written $P(A|B)$ and defined as\n","\n","$$\n","P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n","$$\n","\n","assuming $P(B) > 0$.\n","\n","[Visualization](https://setosa.io/conditional/)\n","\n","\n","**The chain rule** follows from the definition of connditiontal probability:\n","\n","$$\n","P(A \\cap B) = P(A|B)P(B) = P(B|A)P(A)\n","$$\n","\n","\n","Taking one step further, we arrive at the simple but crucial **Bayes' rule**:\n","\n","$$\n","P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n","$$\n","\n","$P(A)$ is often referred to as the **prior**, $P(A|B)$ as the **posterior**, and $P(B|A)$ as the **likelihood**. [Visualization](https://seeing-theory.brown.edu/bayesian-inference/index.html#section1)\n"]}]}